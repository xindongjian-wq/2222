// LLM æœåŠ¡ - NPC æ™ºèƒ½å¯¹è¯
// æ”¯æŒå¤šç§å¤§æ¨¡å‹API

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

export interface NPCChatContext {
  npcName: string;
  npcRole: string;
  npcExpertise: string[];
  npcPersonality: string;
  recentMessages: Array<{ sender: string; content: string; timestamp: number }>;
  currentTopic?: string;
  userTags?: string[];
}

// LLM é…ç½®ç±»å‹
export type LLMProvider = 'openai' | 'anthropic' | 'deepseek' | 'qwen' | 'moonshot' | 'baichuan' | 'zhipu';

export interface LLMConfig {
  provider: LLMProvider;
  apiKey: string;
  baseURL?: string;
  model: string;
  maxTokens?: number;
  temperature?: number;
}

// ç”Ÿæˆ NPC çš„ç³»ç»Ÿæç¤ºè¯
function generateNPCSystemPrompt(context: NPCChatContext): string {
  const { npcName, npcRole, npcExpertise, npcPersonality, userTags } = context;

  let expertiseStr = npcExpertise.join('ã€');
  let userInterests = userTags && userTags.length > 0
    ? `\nç”¨æˆ·å…´è¶£æ ‡ç­¾: ${userTags.join('ã€')}`
    : '';

  return `ä½ æ˜¯ä¸€ä¸ªé»‘å®¢æ¾å‚èµ›è€… NPCï¼Œåå­—å«"${npcName}"ï¼Œè§’è‰²æ˜¯"${npcRole}"ã€‚

ä¸“ä¸šé¢†åŸŸ: ${expertiseStr}
æ€§æ ¼ç‰¹ç‚¹: ${npcPersonality}${userInterests}

å¯¹è¯è¦æ±‚:
1. å›´ç»•é¡¹ç›®å¼€å‘ã€AIæŠ€æœ¯ã€é»‘å®¢æ¾ä¸»é¢˜å‘è¨€
2. å‘è¨€è¦ç®€æ´ï¼ˆ20-50å­—ï¼‰ï¼Œä¸è¦é•¿ç¯‡å¤§è®º
3. å¶å°”ä½¿ç”¨æŠ€æœ¯æœ¯è¯­ï¼Œå±•ç¤ºä¸“ä¸šåº¦
4. è¯­æ°”å‹å¥½ã€ç§¯æï¼Œåƒåœ¨çœŸå®å›¢é˜Ÿè®¨è®º
5. å¯ä»¥èµåŒä»–äººã€æå‡ºå»ºè®®ã€åˆ†äº«ç»éªŒ
6. ç”¨ğŸ¯ğŸ’¡ğŸ¤”ç­‰emojiç‚¹ç¼€ï¼ˆä¸è¦æ¯å¥éƒ½æœ‰ï¼‰
7. æ§åˆ¶å‘è¨€é¢‘ç‡ï¼Œä¸è¦åˆ·å±

è¯é¢˜æ–¹å‘:
- æ¶æ„è®¾è®¡ã€æŠ€æœ¯é€‰å‹
- å‰ç«¯/åç«¯/å…¨æ ˆå¼€å‘
- AI/ML åº”ç”¨
- äº§å“è®¾è®¡ã€ç”¨æˆ·ä½“éªŒ
- å›¢é˜Ÿåä½œã€ä»£ç è´¨é‡
- æµ‹è¯•ã€éƒ¨ç½²ã€è¿ç»´

è¯·æ ¹æ®æœ€è¿‘çš„è®¨è®ºå†…å®¹ï¼Œç”Ÿæˆä¸€å¥è‡ªç„¶çš„å›å¤ã€‚ä¸è¦é‡å¤è¯´è¿‡çš„å†…å®¹ã€‚`;
}

// æ„å»º LLM æ¶ˆæ¯åˆ—è¡¨
function buildMessages(context: NPCChatContext, userMessage?: string): LLMMessage[] {
  const messages: LLMMessage[] = [
    {
      role: 'system',
      content: generateNPCSystemPrompt(context),
    },
  ];

  // æ·»åŠ æœ€è¿‘å‡ æ¡æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆæœ€å¤š5æ¡ï¼‰
  const recentContext = context.recentMessages.slice(-5);
  for (const msg of recentContext) {
    messages.push({
      role: 'assistant',
      content: `${msg.sender}: ${msg.content}`,
    });
  }

  // å¦‚æœæœ‰æ–°çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ å®ƒ
  if (userMessage) {
    messages.push({
      role: 'user',
      content: userMessage,
    });
  } else {
    // æ²¡æœ‰æ–°æ¶ˆæ¯æ—¶ï¼Œæç¤ºç”Ÿæˆè‡ªä¸»å‘è¨€
    messages.push({
      role: 'user',
      content: 'æ ¹æ®æœ€è¿‘çš„è®¨è®ºï¼Œç”Ÿæˆä¸€å¥è‡ªç„¶çš„å‘è¨€å›å¤ã€‚å¦‚æœè®¨è®ºå·²ç»ç»“æŸæˆ–æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¿”å›ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ã€‚',
    });
  }

  return messages;
}

// OpenAI å…¼å®¹ API è°ƒç”¨
async function callOpenAICompatible(
  config: LLMConfig,
  messages: LLMMessage[]
): Promise<LLMResponse> {
  // æ™ºè°±APIç‰¹æ®Šå¤„ç†
  if (config.provider === 'zhipu') {
    return await callZhipuAI(config, messages);
  }

  const baseURL = config.baseURL || 'https://api.openai.com/v1';
  const maxTokens = config.maxTokens || 100;
  const temperature = config.temperature || 0.8;

  const response = await fetch(`${baseURL}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify({
      model: config.model,
      messages,
      max_tokens: maxTokens,
      temperature,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`LLM API error: ${response.status} - ${errorText}`);
  }

  const data = await response.json();
  return {
    content: data.choices[0]?.message?.content || '',
    usage: data.usage ? {
      promptTokens: data.usage.prompt_tokens,
      completionTokens: data.usage.completion_tokens,
      totalTokens: data.usage.total_tokens,
    } : undefined,
  };
}

// æ™ºè°±AI API è°ƒç”¨ï¼ˆç‰¹æ®Šæ ¼å¼ï¼‰
async function callZhipuAI(
  config: LLMConfig,
  messages: LLMMessage[]
): Promise<LLMResponse> {
  const maxTokens = config.maxTokens || 100;
  const temperature = config.temperature || 0.85;

  // æ™ºè°±APIéœ€è¦ç‰¹æ®Šçš„è¯·æ±‚æ ¼å¼
  const payload = {
    model: config.model,
    messages: messages.map(m => ({
      role: m.role === 'system' ? 'system' : m.role === 'assistant' ? 'assistant' : 'user',
      content: m.content,
    })),
    max_tokens: maxTokens,
    temperature,
    stream: false,
  };

  const response = await fetch(`${config.baseURL}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify(payload),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Zhipu AI API error: ${response.status} - ${errorText}`);
  }

  const data = await response.json();
  return {
    content: data.choices[0]?.message?.content || '',
    usage: data.usage ? {
      promptTokens: data.usage.prompt_tokens,
      completionTokens: data.usage.completion_tokens,
      totalTokens: data.usage.total_tokens,
    } : undefined,
  };
}

// ä¸» LLM æœåŠ¡
export const llmService = {
  /**
   * ç”Ÿæˆ NPC å›å¤
   */
  async generateNPCReply(
    config: LLMConfig,
    context: NPCChatContext,
    userMessage?: string
  ): Promise<string> {
    try {
      const messages = buildMessages(context, userMessage);
      const response = await callOpenAICompatible(config, messages);

      let content = response.content.trim();

      // æ¸…ç†å¯èƒ½çš„å¼•å·åŒ…è£¹
      if (content.startsWith('"') && content.endsWith('"')) {
        content = content.slice(1, -1);
      }
      if (content.startsWith("'") && content.endsWith("'")) {
        content = content.slice(1, -1);
      }

      // å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²æˆ–å¤ªçŸ­ï¼Œè¿”å›é»˜è®¤å›å¤
      if (content.length < 3) {
        return '';
      }

      // é™åˆ¶é•¿åº¦ï¼ˆé¿å…å¤ªé•¿ï¼‰
      if (content.length > 100) {
        content = content.slice(0, 97) + '...';
      }

      return content;
    } catch (error) {
      console.error('[LLM] Generate reply error:', error);
      // é™çº§åˆ°ç®€å•å›å¤
      return 'è¿™ä¸ªæƒ³æ³•ä¸é”™ï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥è®¨è®ºä¸€ä¸‹ã€‚';
    }
  },

  /**
   * ç”Ÿæˆ NPC è‡ªä¸»å‘è¨€ï¼ˆä¸éœ€è¦è§¦å‘ï¼‰
   */
  async generateAutonomousMessage(
    config: LLMConfig,
    context: NPCChatContext
  ): Promise<string> {
    return this.generateNPCReply(config, context);
  },

  /**
   * ä»ç¯å¢ƒå˜é‡è·å– LLM é…ç½®
   */
  getConfig(): LLMConfig | null {
    const provider = (process.env.LLM_PROVIDER || 'openai') as LLMProvider;
    const apiKey = process.env.LLM_API_KEY;

    if (!apiKey) {
      console.warn('[LLM] No API key configured');
      return null;
    }

    const defaultModels: Record<LLMProvider, string> = {
      openai: 'gpt-4o-mini',
      anthropic: 'claude-3-haiku-20240307',
      deepseek: 'deepseek-chat',
      qwen: 'qwen-turbo',
      moonshot: 'moonshot-v1-8k',
      baichuan: 'Baichuan4',
      zhipu: 'glm-4-flash',
    };

    const baseUrls: Partial<Record<LLMProvider, string>> = {
      deepseek: 'https://api.deepseek.com/v1',
      qwen: 'https://dashscope.aliyuncs.com/compatible-mode/v1',
      moonshot: 'https://api.moonshot.cn/v1',
      baichuan: 'https://api.baichuan-ai.com/v1',
      zhipu: 'https://open.bigmodel.cn/api/paas/v4',
    };

    return {
      provider,
      apiKey,
      baseURL: baseUrls[provider],
      model: process.env.LLM_MODEL || defaultModels[provider],
      maxTokens: 80,
      temperature: 0.85,
    };
  },

  /**
   * æµ‹è¯• LLM è¿æ¥
   */
  async testConnection(config: LLMConfig): Promise<boolean> {
    try {
      const response = await callOpenAICompatible(config, [
        { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ã€‚' },
        { role: 'user', content: 'ä½ å¥½' },
      ]);
      return response.content.length > 0;
    } catch {
      return false;
    }
  },
};

// é»˜è®¤å¯¼å‡º
export default llmService;
